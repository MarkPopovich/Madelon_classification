{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jupyter Notebook, Step 2 - Identify Features\n",
    "- Build feature selection pipelines using at least three different techniques\n",
    "- **NOTE**: these pipelines are being used for feature selection not prediction\n",
    "    \n",
    "For this portion of the project I will investigate important features in the madelon dataset. \n",
    "\n",
    "One method I will use will be unsupervised learning with a DecisionTreeRegressor against every feature. Second and third, I will use other approaches to attempt to reach the same result as I reach using unsupervised learning, such as SelectKBest and SelectFromModel.\n",
    "\n",
    "For each set of features, I will test the new feature sets against naive models to compare against baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "madelon_file ='madelon_train.csv'\n",
    "madelon_data = []        \n",
    "\n",
    "with open(madelon_file) as f:\n",
    "    readcsv = csv.reader(f, delimiter=' ')\n",
    "    \n",
    "    for row in readcsv:\n",
    "        madelon_data.append(row)\n",
    "        \n",
    "madelon_file_target ='madelon_train_targets.csv'\n",
    "madelon_data_target = []        \n",
    "\n",
    "with open(madelon_file_target) as f:\n",
    "    readcsv = csv.reader(f, delimiter=' ')\n",
    "    \n",
    "    for row in readcsv:\n",
    "        madelon_data_target.append(row)\n",
    "        \n",
    "madelon1 = madelon_data[0:200]\n",
    "\n",
    "madelon_data_df = pd.DataFrame(madelon1)\n",
    "madelon_targets_df = pd.DataFrame(madelon_data_target[0:200])\n",
    "\n",
    "X = madelon_data_df\n",
    "y = madelon_targets_df\n",
    "X['y'] = y\n",
    "\n",
    "X = X.drop([500],axis=1)\n",
    "X['y'] = X['y'].map(int)\n",
    "for column in X.columns:\n",
    "    X[column] = X[column].map(int)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y = X['y']\n",
    "X = X.drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "def calculate_r_2_for_feature(X, feature, model):\n",
    "    tmp_X = X.drop(feature, axis = 1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(tmp_X, X[feature], test_size=0.25)\n",
    "    \n",
    "    regressor = model()\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    score = regressor.score(X_test, y_test)\n",
    "    return score\n",
    "\n",
    "def mean_r2_for_feature(data, feature, model):\n",
    "    scores = []\n",
    "    for _ in range(100):\n",
    "        scores.append(calculate_r_2_for_feature(data, feature, model))\n",
    "        \n",
    "        scores = np.array(scores)\n",
    "        return scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_informative = []\n",
    "for i in range(500):#tqdm(range(500)):\n",
    "    r2 = mean_r2_for_feature(X, i, DecisionTreeRegressor)\n",
    "    if r2 > 0:\n",
    "        scores_informative.append([i,r2])\n",
    "        #print(\"informative found!: \", i, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "information_df = pd.DataFrame(scores_informative)\n",
    "inform_mask = information_df[0]\n",
    "#inform_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_and_score_model_against_raw_and_scaled(model, X_train, X_test, y_train, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    " \n",
    "    model.fit(X_train, y_train)\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_scaled_score = model.score(X_train_scaled, y_train)\n",
    "    test_scaled_score = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    return {\n",
    "            'model': model,\n",
    "            'train_raw_score' : train_score,\n",
    "            'test_raw_score' : test_score,\n",
    "            'train_scaled_score' : train_scaled_score,\n",
    "            'test_scaled_score' : test_scaled_score,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[inform_mask], y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>test_raw_score</th>\n",
       "      <th>test_scaled_score</th>\n",
       "      <th>train_raw_score</th>\n",
       "      <th>train_scaled_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.628571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsRegressor(algorithm='auto', leaf_siz...</td>\n",
       "      <td>0.437321</td>\n",
       "      <td>0.331100</td>\n",
       "      <td>0.485099</td>\n",
       "      <td>0.422394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.864286</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.842857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  test_raw_score  \\\n",
       "0  LogisticRegression(C=1.0, class_weight=None, d...        0.683333   \n",
       "1  KNeighborsRegressor(algorithm='auto', leaf_siz...        0.437321   \n",
       "2  KNeighborsClassifier(algorithm='auto', leaf_si...        0.800000   \n",
       "3  DecisionTreeClassifier(class_weight=None, crit...        0.700000   \n",
       "4  SVC(C=1.0, cache_size=200, class_weight=None, ...        0.633333   \n",
       "\n",
       "   test_scaled_score  train_raw_score  train_scaled_score  \n",
       "0           0.666667         0.678571            0.628571  \n",
       "1           0.331100         0.485099            0.422394  \n",
       "2           0.750000         0.864286            0.821429  \n",
       "3           0.683333         1.000000            1.000000  \n",
       "4           0.866667         1.000000            0.842857  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "results = [fit_and_score_model_against_raw_and_scaled(LogisticRegression(), X_train, X_test, y_train, y_test),\n",
    "           fit_and_score_model_against_raw_and_scaled(KNeighborsRegressor(), X_train, X_test, y_train, y_test),\n",
    "           fit_and_score_model_against_raw_and_scaled(KNeighborsClassifier(), X_train, X_test, y_train, y_test),\n",
    "           fit_and_score_model_against_raw_and_scaled(DecisionTreeClassifier(), X_train, X_test, y_train, y_test),\n",
    "           fit_and_score_model_against_raw_and_scaled(SVC(), X_train, X_test, y_train, y_test)]\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning methods clearly enhance the ability of the models to predict for the dataset. Even with the very small dataset, the model now performs noticably better. \n",
    "\n",
    "Next I will use SelectFromModel and SelectKBest to choose features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, \\\n",
    "                                      SelectFromModel, \\\n",
    "                                      RFE, SelectPercentile\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=20, score_func=<function f_classif at 0x7f6ce0d75268>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb = SelectKBest(k=20)\n",
    "\n",
    "skb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skb_feats = np.where(skb.get_support())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 32,  34,  40,  47,  48,  70, 105, 128, 193, 235, 282, 378, 380,\n",
       "       402, 415, 417, 420, 435, 474, 477])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb_feats.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb.pvalues_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[skb_feats], y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>test_raw_score</th>\n",
       "      <th>test_scaled_score</th>\n",
       "      <th>train_raw_score</th>\n",
       "      <th>train_scaled_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.735714</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsRegressor(algorithm='auto', leaf_siz...</td>\n",
       "      <td>-0.125359</td>\n",
       "      <td>-0.122488</td>\n",
       "      <td>0.259604</td>\n",
       "      <td>0.270457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.757143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  test_raw_score  \\\n",
       "0  LogisticRegression(C=1.0, class_weight=None, d...        0.633333   \n",
       "1  KNeighborsRegressor(algorithm='auto', leaf_siz...       -0.125359   \n",
       "2  KNeighborsClassifier(algorithm='auto', leaf_si...        0.566667   \n",
       "3  DecisionTreeClassifier(class_weight=None, crit...        0.583333   \n",
       "4  SVC(C=1.0, cache_size=200, class_weight=None, ...        0.633333   \n",
       "\n",
       "   test_scaled_score  train_raw_score  train_scaled_score  \n",
       "0           0.716667         0.735714            0.714286  \n",
       "1          -0.122488         0.259604            0.270457  \n",
       "2           0.583333         0.700000            0.757143  \n",
       "3           0.600000         1.000000            1.000000  \n",
       "4           0.733333         1.000000            0.885714  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [fit_and_score_model_against_raw_and_scaled(LogisticRegression(), X_train, X_test, y_train, y_test),\n",
    "           fit_and_score_model_against_raw_and_scaled(KNeighborsRegressor(), X_train, X_test, y_train, y_test),\n",
    "           fit_and_score_model_against_raw_and_scaled(KNeighborsClassifier(), X_train, X_test, y_train, y_test),\n",
    "           fit_and_score_model_against_raw_and_scaled(DecisionTreeClassifier(), X_train, X_test, y_train, y_test),\n",
    "           fit_and_score_model_against_raw_and_scaled(SVC(), X_train, X_test, y_train, y_test)]\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SelectKBest clearly under performs unsupervised learning when selecting 20 best features. \n",
    "\n",
    "A notable exception is LogisticRegression, which actually shows signs of doing slightly better. It should be noted, however, that different train test splits were used in these two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the third feature selection mechanism, I will use SelectFromModel with a Support Vector Machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)\n",
    "\n",
    "sfm = SelectFromModel(LogisticRegression(), threshold='2*mean')\n",
    "sfm.fit(X_train_scaled, y_train)\n",
    "sfm_feats = np.where(sfm.get_support())[0]\n",
    "#sfm_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm_feats.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>test_raw_score</th>\n",
       "      <th>test_scaled_score</th>\n",
       "      <th>train_raw_score</th>\n",
       "      <th>train_scaled_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsRegressor(algorithm='auto', leaf_siz...</td>\n",
       "      <td>-0.234450</td>\n",
       "      <td>-0.168421</td>\n",
       "      <td>0.498363</td>\n",
       "      <td>0.514040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>0.878571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  test_raw_score  \\\n",
       "0  LogisticRegression(C=1.0, class_weight=None, d...        0.550000   \n",
       "1  KNeighborsRegressor(algorithm='auto', leaf_siz...       -0.234450   \n",
       "2  KNeighborsClassifier(algorithm='auto', leaf_si...        0.550000   \n",
       "3  DecisionTreeClassifier(class_weight=None, crit...        0.500000   \n",
       "4  SVC(C=1.0, cache_size=200, class_weight=None, ...        0.633333   \n",
       "\n",
       "   test_scaled_score  train_raw_score  train_scaled_score  \n",
       "0           0.533333         1.000000            1.000000  \n",
       "1          -0.168421         0.498363            0.514040  \n",
       "2           0.600000         0.871429            0.878571  \n",
       "3           0.466667         1.000000            1.000000  \n",
       "4           0.583333         1.000000            1.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[sfm_feats], y, test_size=.3, random_state=42)\n",
    "\n",
    "results = [fit_and_score_model_against_raw_and_scaled(LogisticRegression(), X_train, X_test, y_train, y_test),\n",
    "           fit_and_score_model_against_raw_and_scaled(KNeighborsRegressor(), X_train, X_test, y_train, y_test),\n",
    "           fit_and_score_model_against_raw_and_scaled(KNeighborsClassifier(), X_train, X_test, y_train, y_test),\n",
    "           fit_and_score_model_against_raw_and_scaled(DecisionTreeClassifier(), X_train, X_test, y_train, y_test),\n",
    "           fit_and_score_model_against_raw_and_scaled(SVC(), X_train, X_test, y_train, y_test)]\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select from model does not clearly show any improvement on the naive models at this point. After doing some tuning to the selection mechanism (threshold), I still do not see any marked improvement at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supports = [inform_mask,skb_feats,sfm_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0      28\n",
       " 1      48\n",
       " 2      64\n",
       " 3     105\n",
       " 4     128\n",
       " 5     153\n",
       " 6     241\n",
       " 7     281\n",
       " 8     318\n",
       " 9     336\n",
       " 10    338\n",
       " 11    378\n",
       " 12    433\n",
       " 13    442\n",
       " 14    451\n",
       " 15    453\n",
       " 16    455\n",
       " 17    472\n",
       " 18    475\n",
       " 19    493\n",
       " Name: 0, dtype: int64,\n",
       " array([ 32,  34,  40,  47,  48,  70, 105, 128, 193, 235, 282, 378, 380,\n",
       "        402, 415, 417, 420, 435, 474, 477]),\n",
       " array([  1,  32,  34,  40,  43,  47,  51,  55,  70,  73,  75,  80,  83,\n",
       "         85,  93, 111, 126, 131, 141, 155, 162, 192, 193, 196, 200, 207,\n",
       "        209, 213, 218, 231, 287, 295, 299, 306, 376, 387, 389, 395, 407,\n",
       "        415, 417, 418, 420, 424, 430, 435, 441, 452, 461, 463, 473, 476])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('supports.pkl', 'wb') as f:\n",
    "    pickle.dump(supports, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
